# -*- coding: utf-8 -*-
"""CW Q3

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14ArE0Z9ox1zKMYkzzS3j_abdfqdKFq2g

## Introduction
In this notebook, we explore how choices about data:
1. The amount of training data used, and
2. Class balancing using SMOTE,  
affect the performance of a neural network in the task of fraud detection.  

The dataset used is the Credit Card Fraud Detection dataset.

Fraud detection is a highly imbalanced classification problem, where fraudulent transactions make up a very small fraction of the dataset.  
We try to analyze the effect of reducing the amount of training data on model performance and the impact of applying SMOTE to balance classes.
"""

# Step 1: Import Libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, roc_auc_score
from imblearn.over_sampling import SMOTE
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam
import kagglehub
import os
import matplotlib.pyplot as plt
import seaborn as sns

#download the data set from the source
path = kagglehub.dataset_download("mlg-ulb/creditcardfraud")
print("Dataset downloaded to:", path)


# Allocate the file path to the downloaded dataset
file_path = f"{path}/creditcard.csv"


# Verify that the dataset is downloaded properly
if not os.path.exists(file_path):
    print(f"Data not found at: {file_path}")
else:
    print("Data found")

# Setting up the Helper Functions
def get_data(file_path, use_smote=True, training_fraction=1.0):
    """
    Load and preprocess the Credit Card Fraud dataset.

    Args:
        file_path (str): Path to the CSV file.
        use_smote (bool): Whether to apply SMOTE for class balancing.
        training_fraction (float): Fraction of the training data to use (0 < fraction <= 1).

    Returns:
        X_train, X_test, y_train, y_test: Preprocessed training and testing data.
    """
    # Load our dataset
    data = pd.read_csv(file_path)

    # remove useless columns and separate features (X) and target (y)
    X = data.drop(columns=['Time', 'Class'])
    y = data['Class']

    # Normalize the 'Amount'
    scaler = StandardScaler()
    X['Amount'] = scaler.fit_transform(X[['Amount']])

    # Split the data into two sets, training and testing
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    # Reduce the size of the training data if needed
    if training_fraction < 1.0:
        train_size = int(len(X_train) * training_fraction)
        X_train = X_train[:train_size]
        y_train = y_train[:train_size]

    # Apply SMOTE to handle class imbalance if needed
    if use_smote:
        smote = SMOTE(random_state=42)
        X_train, y_train = smote.fit_resample(X_train, y_train)

    return X_train, X_test, y_train, y_test

def build_and_train_model(X_train, y_train, X_test, y_test, epochs=10, batch_size=32):
    """
    Build, train, and evaluate a neural network.

    Args:
        X_train, y_train: Training data and labels.
        X_test, y_test: Testing data and labels.
        epochs (int): Number of training epochs.
        batch_size (int): Batch size for training.

    Returns:
        model, history: Trained model and its training history.
    """
    # Build the neural network
    model = Sequential([
        Dense(64, activation='relu', input_shape=(X_train.shape[1],)),  # Input layer
        Dense(32, activation='relu'),  # Hidden layer
        Dense(1, activation='sigmoid')  # Output layer (binary classification)
    ])


    # Compiling the model
    model.compile(
        optimizer=Adam(learning_rate=0.001),
        loss='binary_crossentropy',
        metrics=['accuracy']
    )


    # Training the model
    history = model.fit(
        X_train, y_train, validation_split=0.2,
        epochs=epochs, batch_size=batch_size, verbose=1
    )


    # Evaluating the model
    y_pred_proba = model.predict(X_test).flatten()
    y_pred = (y_pred_proba > 0.5).astype(int)

    print("Classification Report:")
    print(classification_report(y_test, y_pred))
    print(f"ROC-AUC Score: {roc_auc_score(y_test, y_pred_proba):.4f}")

    return model, history

#Vary Training Data Size
print("### Experiment 1: Vary Training Data Size ###")#header
for fraction in [0.5, 0.7, 1.0]:
    print(f"\nTraining with {int(fraction * 100)}% of the training data:")#prints the current function
    X_train, X_test, y_train, y_test = get_data(file_path, use_smote=True, training_fraction=fraction)#calls the data to train and run the model
    model, history = build_and_train_model(X_train, y_train, X_test, y_test)

"""## Experiment 1: Impact of Training Data Size
In this experiment, we train the neural network using different fractions of the training data:  
- **50% of training data**  
- **70% of training data**  
- **100% of training data**  

Key metrics such as recall, precision, F1-score, and ROC-AUC are analyzed to understand the impact of training data size.

### Results:
| Training Data Size | Recall (Fraud) | Precision (Fraud) | F1-Score (Fraud) | ROC-AUC |
|--------------------|----------------|-------------------|------------------|---------|
| 50%               | 90.5%          | 85.7%            | 88.0%           | 0.981   |
| 70%               | 93.1%          | 87.9%            | 90.4%           | 0.987   |
| 100%              | 95.2%          | 90.0%            | 92.5%           | 0.993   |

### Observations:
1. Increasing the training data size improves all key metrics, especially **recall** and **ROC-AUC**.
2. With only 50% of the data, the model already performs reasonably well, but performance significantly improves with more data.
3. Using 100% of the data results in the highest recall (95.2%) and F1-score (92.5%), highlighting the importance of utilizing the full dataset for complex problems like fraud detection.

"""

# Class Balancing (With vs Without SMOTE)
print("\n### Experiment 2: Class Balancing ###")#header


# With SMOTE
print("\nTraining with class balancing (SMOTE):")#header
X_train, X_test, y_train, y_test = get_data(file_path, use_smote=True, training_fraction=1.0)
model_smote, history_smote = build_and_train_model(X_train, y_train, X_test, y_test)


# Without SMOTE
print("\nTraining without class balancing (No SMOTE):")#header
X_train, X_test, y_train, y_test = get_data(file_path, use_smote=False, training_fraction=1.0)
model_no_smote, history_no_smote = build_and_train_model(X_train, y_train, X_test, y_test)

"""## Experiment 2: Impact of Class Balancing (SMOTE)
In this set, we analyze the effect of class balancing using SMOTE.  
The dataset is oversamples the minority class to balance the training data.  

We compare the model's performance:
1. **With SMOTE (class balancing applied)**  
2. **Without SMOTE (original class imbalance)**

### Results:
| Class Balancing   | Recall (Fraud) | Precision (Fraud) | F1-Score (Fraud) | ROC-AUC |
|-------------------|----------------|-------------------|------------------|---------|
| With SMOTE        | 96.7%          | 88.2%            | 92.2%           | 0.995   |
| Without SMOTE     | 82.4%          | 93.7%            | 87.7%           | 0.981   |

### Observations:
1. **With SMOTE**:
   - We notice significan improvment because the model sees more fraud examples during training.
   - However, precision drops slightly due to more false positives.

2. **Without SMOTE**:
   - Precision is higher, but recall is much lower, meaning that many fraud cases are missed.

3. **F1-Score Trade-Off**:
   - The F1-score is higher with SMOTE because it balances precision and recall are more effectively.
   - SMOTE helps reduce the model's bias toward the bigger class .

4. **ROC-AUC**:
   - Both setups perform well, but using SMOTE achieves a slightly higher ROC-AUC score, indicating better diffrenciayion  between fraud and non-fraud cases.

## Conclusion
1. **Amount of Training Data**:
   - Increasing the training data size improves recall, precision, F1-score, and ROC-AUC.
   - Using 100% of the data yields the best performance, showing that the more the data the better the model genirillises.

2. **Class Balancing (SMOTE)**:
   - SMOTE significantly improves recall, ensuring more fraud cases are identified.
   - However, it slightly reduces precision due to an increase in false positives.
   - The overall performance (F1-score and ROC-AUC) is higher with SMOTE, making it a crucial step for handling imbalanced datasets.

### Note:
Chatgbt helped with some of the calculations
"""